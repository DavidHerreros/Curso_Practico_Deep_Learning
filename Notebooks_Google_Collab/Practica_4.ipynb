{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  # Cambiar cu118 por la versión de Cuda del sistema\n",
    "!pip install ipywidgets widgetsnbextension torchsummary einops tqdm matplotlib\n",
    "!pip install -U \"jax[cuda12]\"  # Cambiar cuda11 por la versión de nuestro CUDA\n",
    "!pip install flax\n",
    "!pip install -q clu\n",
    "!jupyter contrib nbextension install --user"
   ],
   "id": "3b1e7fb678e85333"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PRÁCTICA 1: MODELOS DE DIFUSION Y MECANISMOS DE ATENCIÓN\n",
    "\n",
    "En esta práctica veremos como podemos definir una de las arquitecturas más novedosas en el campo de las redes neuronales: los **__modelos de difusión__**.\n",
    "\n",
    "En esencia, un modelo de difusión aprende a generar datos similares a una entrada a partir de imágenes con puro ruido. Este tipo de modelos se han vuelto muy populares en la generación de imágenes como DALL-E de OpenAI.\n",
    "\n",
    "Los principales conceptos que cubriremos con esta práctica son los siguientes:\n",
    " - Definir modelos de difusión\n",
    " - Arquitectura U-NET\n",
    " - Mecanismos de atención basados en __visual transformers__\n",
    "\n",
    "Antes de comenzar con el modelo, vamos a importar las librerías que necesitaremos:"
   ],
   "id": "9bcc84329cf1e9b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:07:06.166743Z",
     "start_time": "2025-05-05T09:07:05.011137Z"
    }
   },
   "cell_type": "code",
   "source": "!jupyter nbextension enable --py widgetsnbextension",
   "id": "ecb6b75ed4bc07a8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:07:06.175857Z",
     "start_time": "2025-05-05T09:07:06.172423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from typing import List\n",
    "import random\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "# Librerías de PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Una librería muy util para ajustar los tamaños de nuestros tensores\n",
    "from einops import rearrange\n",
    "\n",
    "# Torch vision para descargar y preprocesar el conjunto de datos que usaremos para entrenar la red\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Herramientas adicionales para la visualización de los datos y resultados\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np"
   ],
   "id": "c37b5457ba1e6f83",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Durante esta práctica usaremos de nuevo el conjunto de datos `FashionMNIST` para entrenar la red. Comenzaremos por descargar y generar el `DataLoader` para poder enviar datos a la red de manera eficiente:",
   "id": "3269d3b461b27640"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:07:06.225797Z",
     "start_time": "2025-05-05T09:07:06.191864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = datasets.FashionMNIST(\"./MNIST_DATA\", train=True, download=True,transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True, num_workers=4)"
   ],
   "id": "dfa1df19591a912b",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "También podemos aprovechar para visualizar los datos como hemos hecho en prácticas anteriores:",
   "id": "7888f7a908e51213"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:07:16.488885Z",
     "start_time": "2025-05-05T09:07:06.229818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Inspección de los datos de entrenamiento: \")\n",
    "for _, data in enumerate(train_loader):\n",
    "    print(\"Tamaño del lote: \", data[0].shape)\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(10, 4))\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[i].imshow(data[0][i].squeeze(), cmap=\"gray\")\n",
    "        ax[i].axis(\"off\")\n",
    "    plt.show()\n",
    "    # Hacemos break para no recorrer todo el loader\n",
    "    break"
   ],
   "id": "343dd408272819c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspección de los datos de entrenamiento: \n",
      "Tamaño del lote:  torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x400 with 4 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Antes de definir nuestra red, vamos a generar un embedding sinusoidal. Este tipo de embeddings permiten transmitir el concepto de \"secuencia\" a nuestros datos:\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/0*DEZ4oDAZm0RCOPDb.png\" width=500, heigh=500 />\n",
    "\n",
    "En nuestro caso, los usaremos para definir diferentes momentos en el tiempo: cuando nuestro tiempo sea igual a 0, tendremos imágenes compuestas completamente por ruido. A medida que el tiempo avance, nuestras imágenes iran perdiendo ruido y ganando señal hasta llegar a una imagen limpia del objeto de interés. Por lo tanto, este concepto de tiempo no deja de ser una secuencia de pasos, teniendo cada uno una cantidad de ruido menor.\n"
   ],
   "id": "85c61b2a4d130b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:07:16.500536Z",
     "start_time": "2025-05-05T09:07:16.495937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SinusoidalEmbeddings(nn.Module):\n",
    "    def __init__(self, time_steps:int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        position = torch.arange(time_steps).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))\n",
    "        embeddings = torch.zeros(time_steps, embed_dim, requires_grad=False)\n",
    "        embeddings[:, 0::2] = torch.sin(position * div)\n",
    "        embeddings[:, 1::2] = torch.cos(position * div)\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        embeds = self.embeddings[t].to(x.device)\n",
    "        return embeds[:, :, None, None]"
   ],
   "id": "129ba2c2f5030591",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Una vez definidos nuestro embedding con sentido de secuencia, podemos comenzar a instanciar nuestra U-NET. En primer lugar, comenzaremos por definir el bloque esencial que compone este tipo de redes: un conjunto de capas convolucionales con conexiones residuales:",
   "id": "3aa56c38b106acaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:07:16.512633Z",
     "start_time": "2025-05-05T09:07:16.508472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, C: int, num_groups: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.gnorm1 = nn.GroupNorm(num_groups=num_groups, num_channels=C)  # Esta capa normaliza los canales dividiendolas previamente en pequeños grupos\n",
    "        self.gnorm2 = nn.GroupNorm(num_groups=num_groups, num_channels=C)  # Esta capa normaliza los canales dividiendolas previamente en pequeños grupos\n",
    "        self.conv1 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob, inplace=True)\n",
    "\n",
    "    def forward(self, x, embeddings):\n",
    "        x = x + embeddings[:, :x.shape[1], :, :]  # Embeddings vendrá definido por SinusoidalEmbeddings en un \"momento\" determinado de la secuencia de tiempo\n",
    "        r = self.conv1(self.relu(self.gnorm1(x)))\n",
    "        r = self.dropout(r)\n",
    "        r = self.conv2(self.relu(self.gnorm2(r)))\n",
    "        return r + x"
   ],
   "id": "97a85d872c3c880f",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://api.wandb.ai/files/wandb_fc/images/projects/605819/472b8f50.png\" width=500, heigh=500 />\n",
   "id": "1e903b92944bd7c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Una vez definido el bloque esencial de nuezstra U-NET, podemos pasar a definir cada capa de nuestra red neuronal. Estas capas estarán compuestas por dos bloques residuales (previamente definidos), una capa de atención basada en transformadores visuales, y una capa de convolución/convolución transpuesta (esto dependerá de si esta capa se encuentra en la primera o en la segunda mitad de nuestra U-NET tal y como veremos más adelante):",
   "id": "1f27dff98d89e4d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:07:16.534664Z",
     "start_time": "2025-05-05T09:07:16.529888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UnetLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "            upscale: bool,\n",
    "            attention: bool,\n",
    "            num_groups: int,\n",
    "            dropout_prob: float,\n",
    "            num_heads: int,\n",
    "            C: int):\n",
    "        super().__init__()\n",
    "        self.ResBlock1 = ResBlock(C=C, num_groups=num_groups, dropout_prob=dropout_prob)\n",
    "        self.ResBlock2 = ResBlock(C=C, num_groups=num_groups, dropout_prob=dropout_prob)\n",
    "        if upscale:\n",
    "            self.conv = nn.ConvTranspose2d(C, C // 2, kernel_size=4, stride=2, padding=1)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(C, C * 2, kernel_size=3, stride=2, padding=1)\n",
    "        if attention:\n",
    "            self.attention_layer = Attention(C, num_heads=num_heads, dropout_prob=dropout_prob)\n",
    "\n",
    "    def forward(self, x, embeddings):\n",
    "        x = self.ResBlock1(x, embeddings)\n",
    "        if hasattr(self, 'attention_layer'):\n",
    "            x = self.attention_layer(x)\n",
    "        x = self.ResBlock2(x, embeddings)\n",
    "        return self.conv(x), x"
   ],
   "id": "2bec573c4cac683f",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para completar la capa de nuestra U-NET, tenemos que definir la capa con mecanismo de atención. Para más información sobre el mecanismo de funcionamiento de esta capa, es recomendable dirigirse a las diapositivas teóricas del curso.\n",
    "\n",
    "A modo de resumen, la capa de atención se basa en calcular una sería de matrices entrenables a partir de los inputs:\n",
    " - Capa **Q** (Queries): Representa el conjunto de elementos a partir de los cuales calcularemos la atención.\n",
    " - Capa **K** (Keys): Una capa que contiene \"identificadores\" para un determinado valor. Se usa en conjunto con la capa **Q** para determinar cuanta atención hay que prestar a cada valor.\n",
    " - Capa **V** (Values): Esta capa contiene los valores con información (es decir, nuestro input) a los que aplicaremos la atención previamente calculada.\n",
    "\n",
    "Nuestra capa de atención calculará las matrices Q, K y V a partir de los inputs de la capa, aplicando una capa densa (que se puede entender como una proyección de los inputs en base a una matriz entrenable). Previamente a este cálculo, redimensionaremos nuestro input fusionando las dimensiones espaciales de la imagen (generando así una secuencia 1D sobre la que aplicar la atención) y dejando la capa de canales como una capa auxiliar para calcular las features necesarias para derivar las matrices Q, K y V.\n",
    "\n",
    "Una vez calculadas estas matrices, calcularemos la atención de acuerdo a la fórmula:\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*sXEtwjKCACQ6yfW5T8UolQ.png\" width=500, heigh=500 />\n"
   ],
   "id": "dcff3fcd83a3b2cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:07:16.542739Z",
     "start_time": "2025-05-05T09:07:16.538680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, C: int, num_heads: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.proj1 = nn.Linear(C, C * 3)\n",
    "        self.proj2 = nn.Linear(C, C)\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2:]\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.proj1(x)\n",
    "        x = rearrange(x, 'b L (C H K) -> K b H L C', K=3, H=self.num_heads)\n",
    "        q, k, v = x[0], x[1], x[2]\n",
    "        x = F.scaled_dot_product_attention(q, k, v, is_causal=False, dropout_p=self.dropout_prob)\n",
    "        x = rearrange(x, 'b H (h w) C -> b h w (C H)', h=h, w=w)\n",
    "        x = self.proj2(x)\n",
    "        return rearrange(x, 'b h w C -> b C h w')"
   ],
   "id": "e3d81fc911775ff3",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A partir de las capas previamente definidas, podemos construir nuestra U-Net. Una arquitectura U-NET se basa en una serie de capas simétricas de submuestreado y escalado con conexiones residuales, de tal manera que la salida de la red tenga el mismo tamaño que los datos de entrada. Nuestra clase incluirá un conjunto de listas que nos permitirán definir tanto el número de capas de nuestra U-Net como el encrustamiento de capas de atención solo en determinadas capas.",
   "id": "39c5d5c362f4c88d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:07:16.558013Z",
     "start_time": "2025-05-05T09:07:16.551014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self,\n",
    "            Channels: List = [64, 128, 256, 512, 512, 384],\n",
    "            Attentions: List = [False, True, False, False, False, True],\n",
    "            Upscales: List = [False, False, False, True, True, True],\n",
    "            num_groups: int = 32,\n",
    "            dropout_prob: float = 0.1,\n",
    "            num_heads: int = 8,\n",
    "            input_channels: int = 1,\n",
    "            output_channels: int = 1,\n",
    "            time_steps: int = 1000):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(Channels)\n",
    "        self.shallow_conv = nn.Conv2d(input_channels, Channels[0], kernel_size=3, padding=1)\n",
    "        out_channels = (Channels[-1] // 2)+Channels[0]\n",
    "        self.late_conv = nn.Conv2d(out_channels, out_channels // 2, kernel_size=3, padding=1)\n",
    "        self.output_conv = nn.Conv2d(out_channels // 2, output_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.embeddings = SinusoidalEmbeddings(time_steps=time_steps, embed_dim=max(Channels))\n",
    "        for i in range(self.num_layers):\n",
    "            layer = UnetLayer(\n",
    "                upscale=Upscales[i],\n",
    "                attention=Attentions[i],\n",
    "                num_groups=num_groups,\n",
    "                dropout_prob=dropout_prob,\n",
    "                C=Channels[i],\n",
    "                num_heads=num_heads\n",
    "            )\n",
    "            setattr(self, f'Layer{i+1}', layer)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.shallow_conv(x)\n",
    "        residuals = []\n",
    "        for i in range(self.num_layers//2):\n",
    "            layer = getattr(self, f'Layer{i+1}')\n",
    "            embeddings = self.embeddings(x, t)\n",
    "            x, r = layer(x, embeddings)\n",
    "            residuals.append(r)\n",
    "        for i in range(self.num_layers//2, self.num_layers):\n",
    "            layer = getattr(self, f'Layer{i+1}')\n",
    "            x = torch.concat((layer(x, embeddings)[0], residuals[self.num_layers-i-1]), dim=1)\n",
    "        return self.output_conv(self.relu(self.late_conv(x)))"
   ],
   "id": "91acb9a6f4e6a911",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En este punto, tenemos nuestra red completamente definida y lista para ser entrenada. Sin embargo, aún no tenemos un método para corromper nuestras imágenes con diferentes niveles de ruido dependiendo del momento en el \"tiempo\" en el que se encuentren nuestras imágenes. Recordemos que este \"momento en el tiempo\" hacer referencia al nivel de ruido en las imágenes (siendo t=0 el momento en el que solo hay ruido y t=N el momento en el que nuestra imagen no tiene ruido).\n",
    "\n",
    "Para ello, vamos a definir un ``Scheduler``, el cual nos permitirá recuperar los parámetros necesarios para recuperar el nivel de ruido que corromperá a la imagen en cada momento en el tiempo:"
   ],
   "id": "16a38ae6389d41b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:07:16.572548Z",
     "start_time": "2025-05-05T09:07:16.568547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DDPM_Scheduler(nn.Module):\n",
    "    def __init__(self, num_time_steps: int=1000):\n",
    "        super().__init__()\n",
    "        self.beta = torch.linspace(1e-4, 0.02, num_time_steps, requires_grad=False)\n",
    "        alpha = 1 - self.beta\n",
    "        self.alpha = torch.cumprod(alpha, dim=0).requires_grad_(False)\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.beta[t], self.alpha[t]\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ],
   "id": "78bd69360a834738",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Como se puede observar en la celda anterior, además de nuestro scheduler (``DDPM_Scheduler``) también se incluye la función ``set_seed``. Por norma general, esta función solo la usaremos en el caso de querer reproducir de manera exacta los resultados de un entrenamiento determinado.\n",
    "\n",
    "Es importante recordar en este momento que el proceso de entrenamiento de una red es estocástico, lo que da lugar a resultados con cierta variabilidad en cada entrenamiento. Esto se debe a la presencia de ciertos números aleatorios que son diferentes en cada ejecución. El método ``set_seed`` fuerza a que estos números aleatorios sean los mismos en cada ejecución, permitiendo así reproducir los resultados mientras comprobamos que la red funciona correctamente. Una vez hagamos estas comprobaciones, es importante dejar de usar el método para poder tener variabilidad en nuestros entrenamientos."
   ],
   "id": "2002b486a5133b5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Una vez definidos todos los componentes necesarios para entrenar la red, podemos proceder con la instanciación del loop de entrenamiento. En ese caso, lo encapsularemos dentro de una función para que sea más sencillo de ejecutar en diferentes scripts.\n",
    "\n",
    "Nuestra función de entrenamiento recibirá como input los principales hiperparámetros a controlar para asegurar que el entrenamiento se ajusta a nuestro requirimientos de hardware. Estos parámetros incluyen:\n",
    "\n",
    " - Número de \"momentos en el tiempo\" (**num_time_steps**): Permite determinar cuantos niveles de ruido habrá entre el primer momento en el tiempo (imagen formada solo por ruido) y el tiempo final (imagen sin ruido)\n",
    " - Número de epochs (**num_epochs**): Permite modificar el número de veces que se recorre el conjunto de entrenamiento\n",
    " - **seed**: Este parámetro se usa para controlar la función ``set_seed``, permitiendo así determinar si nuestro entranamiento es estocástico o lo fijamos para que de siempre el mismo resultado. El valor por defecto indica que el proceso será estocástico.\n",
    " - Archivo de punto de guardado (**checkpoint_path**): Permite dar el path a un archivo dónde iremos guardando el estado de la red a medida que el entrenamiento avanza. Esto es muy útil sobre todo cuando el proceso de entrenamiento es largo o queremos analizar el estado de la red en algún punto intermedio del entrenamiento, permitiéndonos recargar el estado de la red en ese punto para hacer inferencia o continuar el entrenamiento desde ese punto."
   ],
   "id": "501bef8125000033"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:07:16.590960Z",
     "start_time": "2025-05-05T09:07:16.584724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(batch_size: int=64,\n",
    "          num_time_steps: int=1000,\n",
    "          num_epochs: int=15,\n",
    "          seed: int=-1,\n",
    "          lr=2e-5,\n",
    "          checkpoint_path: str=None):\n",
    "    set_seed(random.randint(0, 2**32-1)) if seed == -1 else set_seed(seed)\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(\"./MNIST_DATA\", train=True, download=True,transform=transforms.ToTensor())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4)\n",
    "\n",
    "    scheduler = DDPM_Scheduler(num_time_steps=num_time_steps)\n",
    "    model = UNET().cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    if checkpoint_path is not None and os.path.isdir(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['weights'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    elif checkpoint_path is not None and not os.path.isdir(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for bidx, (x,_) in enumerate(tqdm(train_loader, desc=f\"Epoch {i + 1}/{num_epochs}\")):\n",
    "            x = x.cuda()\n",
    "            x = F.pad(x, (2, 2, 2, 2))\n",
    "            t = torch.randint(0, num_time_steps, (batch_size,))\n",
    "            e = torch.randn_like(x, requires_grad=False)\n",
    "            a = scheduler.alpha[t].view(batch_size, 1, 1, 1).cuda()\n",
    "            x = (torch.sqrt(a) * x) + (torch.sqrt(1 - a) * e)\n",
    "            output = model(x, t)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, e)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {i + 1} | Loss {total_loss / (60000 / batch_size):.5f}')\n",
    "\n",
    "    checkpoint = {\n",
    "        'weights': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    if checkpoint_path is not None:\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_path, \"checkpoint.pth\"))"
   ],
   "id": "6340e8721cf18b53",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "El siguiente paso consiste en entrenar nuestra red usando la función anterior:",
   "id": "827bc99cc35977fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:23:49.337082Z",
     "start_time": "2025-05-05T09:07:16.604815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    if os.path.isdir(os.path.join('checkpoints')):\n",
    "        shutil.rmtree(os.path.join('checkpoints'))\n",
    "    train(checkpoint_path=os.path.join('checkpoints', 'ddpm_checkpoint'), lr=1e-4, num_epochs=5)"
   ],
   "id": "89b6e1e5c4babd19",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 937/937 [03:12<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 0.06651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 937/937 [03:23<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss 0.03234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 937/937 [03:09<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss 0.02733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 937/937 [03:14<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss 0.02570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 937/937 [03:32<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss 0.02421\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Una vez hayamos entrenado la red, podemos aprovechar nuestro archivo ``checkpoint.pth`` para recargarla y hacer inferencia de nuestro conjunto de datos para así comprobar si la red es capaz de regenerar nuestras imágenes a partir de puro ruido. Para ello, definiremos en primer lugar una función para representar las imágenes que se generen desde la red:",
   "id": "bef3ab12f8f963c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:33:15.463421Z",
     "start_time": "2025-05-05T09:33:15.457696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_reverse(images: List, times: List=None, save_gif=True):\n",
    "    if times is not None:\n",
    "        t = 0\n",
    "\n",
    "    if save_gif:\n",
    "        frames = []\n",
    "        fig, ax = plt.subplots()\n",
    "        for i in range(len(images)):\n",
    "            x = images[i].squeeze(0)\n",
    "            x = rearrange(x, 'c h w -> h w c')\n",
    "            x = x.numpy()\n",
    "            im = ax.imshow(x, animated=True, cmap=\"Grays_r\")\n",
    "            if i == 0:\n",
    "                ax.imshow(x, cmap=\"Grays_r\")\n",
    "            if times is not None:\n",
    "                ax.set_title(\"Time: \" + str(times[t]))\n",
    "                t += 1\n",
    "            ax.axis('off')\n",
    "            fig.tight_layout()\n",
    "            frames.append([im])\n",
    "        ani = animation.ArtistAnimation(fig, frames, interval=200, blit=True,\n",
    "                                        repeat_delay=1000)\n",
    "        writer = animation.PillowWriter(fps=2, bitrate=1800)\n",
    "        ani.save(f\"diffusion_model_example.gif\", writer=writer)\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, len(images), figsize=(len(images), 1))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            x = images[i].squeeze(0)\n",
    "            x = rearrange(x, 'c h w -> h w c')\n",
    "            x = x.numpy()\n",
    "            ax.imshow(x, cmap=\"Grays\")\n",
    "            if times is not None:\n",
    "                ax.set_title(\"Time: \" + str(times[t]))\n",
    "                t += 1\n",
    "            ax.axis('off')\n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "58abc8e7a4f85884",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A continuación definiremos una nueva función que nos permitirá hacer inferencia con la red. Esta función aprovechará el archivo ``checkpoint.pth`` para recargar el estado de la red y así poder utilizarla para predecir imágenes a partir de puro ruido.\n",
    "\n",
    "Para ello, aprovecharemos nuestro ``DDPM_Scheduler`` para generar los parámetros que necesitaremos para generar los diferentes niveles de ruido. De manera intuitiva, el proceso de inferencia es exactamente igual al de entrenamiento pero en sentido opuesto: empezaremos con imágenes de ruido y progresivamente la red irá disminuyendo el ruido hasta conseguir una imagen única que se parezca a las imágenes de entrenamiento. Cada vez que generemos una nueva imagen de ruido, obtendremos una imagen única diferente."
   ],
   "id": "677c1cbf74c7d3fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:33:17.720213Z",
     "start_time": "2025-05-05T09:33:17.714471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def inference(checkpoint_path: str=None,\n",
    "              num_time_steps: int=1000):\n",
    "    checkpoint = torch.load(os.path.join(checkpoint_path, \"checkpoint.pth\"))\n",
    "    model = UNET().cuda()\n",
    "    model.load_state_dict(checkpoint['weights'])\n",
    "    scheduler = DDPM_Scheduler(num_time_steps=num_time_steps)\n",
    "    times = [0,15,50,100,200,300,400,550,700,999]\n",
    "    images = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        for i in range(1):\n",
    "            z = torch.randn(1, 1, 32, 32)\n",
    "            for t in reversed(range(1, num_time_steps)):\n",
    "                t = [t]\n",
    "                temp = (scheduler.beta[t] / ( (torch.sqrt( 1 - scheduler.alpha[t])) * (torch.sqrt(1 - scheduler.beta[t]))))\n",
    "                z = (1 / (torch.sqrt(1 - scheduler.beta[t]))) * z - (temp * model(z.cuda(), t).cpu())\n",
    "                if t[0] in times:\n",
    "                    images.append(z)\n",
    "                e = torch.randn(1, 1, 32, 32)\n",
    "                z = z + (e * torch.sqrt(scheduler.beta[t]))\n",
    "            temp = scheduler.beta[0] / ((torch.sqrt(1 - scheduler.alpha[0])) * (torch.sqrt(1 - scheduler.beta[0])))\n",
    "            x = (1 / (torch.sqrt(1 - scheduler.beta[0]))) * z - (temp * model(z.cuda(), [0]).cpu())\n",
    "\n",
    "            images.append(x)\n",
    "            display_reverse(images, times=list(reversed(times)))\n",
    "            images = []"
   ],
   "id": "f4d5f7cd21fcb20e",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A continuación, probaremos la capacidad de inferencia de la red:",
   "id": "6aa39e3bd2ac8dd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:33:29.352241Z",
     "start_time": "2025-05-05T09:33:21.166209Z"
    }
   },
   "cell_type": "code",
   "source": "inference(os.path.join('checkpoints', 'ddpm_checkpoint'))",
   "id": "3019333251290ffc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"diffusion_model_example.gif\" width=500, heigh=500 />",
   "id": "54fee1589c919b44"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
