{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  # Cambiar cu118 por la versión de Cuda del sistema\n",
    "!pip install ipywidgets widgetsnbextension torchsummary einops tqdm matplotlib\n",
    "!pip install -U \"jax[cuda12]\"  # Cambiar cuda11 por la versión de nuestro CUDA\n",
    "!pip install flax\n",
    "!pip install -q clu"
   ],
   "id": "bb566a38e3da6524"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PRÁCTICA 1: MODELOS DE DIFUSION Y MECANISMOS DE ATENCIÓN\n",
    "\n",
    "En esta práctica veremos como podemos definir una de las arquitecturas más novedosas en el campo de las redes neuronales: los **__modelos de difusión__**.\n",
    "\n",
    "En esencia, un modelo de difusión aprende a generar datos similares a una entrada a partir de imágenes con puro ruido. Este tipo de modelos se han vuelto muy populares en la generación de imágenes como DALL-E de OpenAI.\n",
    "\n",
    "Los principales conceptos que cubriremos con esta práctica son los siguientes:\n",
    " - Definir modelos de difusión\n",
    " - Arquitectura U-NET\n",
    " - Mecanismos de atención basados en __visual transformers__\n",
    "\n",
    "Antes de comenzar con el modelo, vamos a importar las librerías que necesitaremos:"
   ],
   "id": "b097dc4757668758"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T07:51:14.527079Z",
     "start_time": "2025-05-12T07:51:14.048632Z"
    }
   },
   "cell_type": "code",
   "source": "!jupyter nbextension enable --py widgetsnbextension",
   "id": "ecb6b75ed4bc07a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dherreros/miniforge3/envs/pytorch/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001B[32mOK\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A diferencia de la **Práctica 4**, aquí veremos como usar las funciones de compilación de código en __JAX__ y __XLA__ para mejorar aún la eficiencia de nuestras redes.",
   "id": "7d5aeffe54035bc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T07:51:14.544325Z",
     "start_time": "2025-05-12T07:51:14.540334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from typing import List\n",
    "import shutil\n",
    "\n",
    "# Librerías de JAX\n",
    "import jax\n",
    "from jax import random as jnr, numpy as jnp\n",
    "from jax.tree_util import tree_map\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "import orbax.checkpoint\n",
    "from flax.training import orbax_utils\n",
    "from flax.struct import field\n",
    "\n",
    "# Una librería muy util para ajustar los tamaños de nuestros tensores\n",
    "from einops import rearrange\n",
    "\n",
    "# Torch vision para descargar y preprocesar el conjunto de datos que usaremos para entrenar la red\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "\n",
    "# Herramientas adicionales para la visualización de los datos y resultados\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np"
   ],
   "id": "c37b5457ba1e6f83",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Durante esta práctica usaremos de nuevo el conjunto de datos `FashionMNIST` para entrenar la red. Comenzaremos por descargar y generar el `DataLoader` para poder enviar datos a la red de manera eficiente.\n",
    "\n",
    "Dado que vamos a aprovechar el ``DataLoader`` de __PyTorch__ en __JAX__, necesitamos convertir nuestros lotes a __Numpy__ par que __JAX__ pueda trabajar con los datos. Esto lo haremos usando la funcionalidad ``collate_fn`` del ``DataLoader`` junto con una función que se encargue de hacer esta conversión:"
   ],
   "id": "3269d3b461b27640"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T07:51:15.003635Z",
     "start_time": "2025-05-12T07:51:14.589775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def numpy_collate(batch):\n",
    "  \"\"\"\n",
    "  Esta funión determina como combinar una lista de muestras en un lote.\n",
    "  default_collate crea tensores de PyTorch, y tree_map se encarga de convertirlos a numpy arrays.\n",
    "  \"\"\"\n",
    "  return tree_map(np.asarray, default_collate(batch))\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\"./MNIST_DATA\", train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True, num_workers=0, collate_fn=numpy_collate)"
   ],
   "id": "dfa1df19591a912b",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vamos a definir también el dispositivo que usaremos para procesar (GPU):",
   "id": "1001f80568a957b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T07:51:15.019193Z",
     "start_time": "2025-05-12T07:51:15.016457Z"
    }
   },
   "cell_type": "code",
   "source": "DEVICE = jax.devices('gpu')[0]",
   "id": "b56473dcb3bda24d",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "También podemos aprovechar para visualizar los datos como hemos hecho en prácticas anteriores:",
   "id": "7888f7a908e51213"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T07:51:15.120471Z",
     "start_time": "2025-05-12T07:51:15.070797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Inspección de los datos de entrenamiento: \")\n",
    "for _, data in enumerate(train_loader):\n",
    "    x = rearrange(data[0], 'b c w h -> b w h c')\n",
    "    print(\"Tamaño del lote: \",x.shape)\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(10, 4))\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[i].imshow(x[i].squeeze(), cmap=\"gray\")\n",
    "        ax[i].axis(\"off\")\n",
    "    plt.show()\n",
    "    # Hacemos break para no recorrer todo el loader\n",
    "    break"
   ],
   "id": "343dd408272819c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspección de los datos de entrenamiento: \n",
      "Tamaño del lote:  (64, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x400 with 4 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Antes de definir nuestra red, vamos a generar un embedding sinusoidal. Este tipo de embeddings permiten transmitir el concepto de \"secuencia\" a nuestros datos:\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/0*DEZ4oDAZm0RCOPDb.png\" width=500, heigh=500 />\n",
    "\n",
    "En nuestro caso, los usaremos para definir diferentes momentos en el tiempo: cuando nuestro tiempo sea igual a 0, tendremos imágenes compuestas completamente por ruido. A medida que el tiempo avance, nuestras imágenes iran perdiendo ruido y ganando señal hasta llegar a una imagen limpia del objeto de interés. Por lo tanto, este concepto de tiempo no deja de ser una secuencia de pasos, teniendo cada uno una cantidad de ruido menor.\n"
   ],
   "id": "85c61b2a4d130b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T07:51:15.145116Z",
     "start_time": "2025-05-12T07:51:15.140510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SinusoidalEmbeddings(nn.Module):\n",
    "    time_steps: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        position = jnp.arange(self.time_steps, dtype=jnp.float32)[:, None]\n",
    "        div = jnp.exp(jnp.arange(0, self.embed_dim, 2, dtype=jnp.float32) * -(jnp.log(10000.0) / self.embed_dim))\n",
    "        embeddings = jnp.zeros((self.time_steps, self.embed_dim))\n",
    "        embeddings = embeddings.at[:, 0::2].set(jnp.sin(position * div))\n",
    "        embeddings = embeddings.at[:, 1::2].set(jnp.cos(position * div))\n",
    "        self.embeddings = jnp.array(embeddings)\n",
    "\n",
    "    def __call__(self, t):\n",
    "        embeds = self.embeddings[t]\n",
    "        return embeds[:, None, None, :]"
   ],
   "id": "129ba2c2f5030591",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Una vez definidos nuestro embedding con sentido de secuencia, podemos comenzar a instanciar nuestra U-NET. En primer lugar, comenzaremos por definir el bloque esencial que compone este tipo de redes: un conjunto de capas convolucionales con conexiones residuales:",
   "id": "3aa56c38b106acaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T07:51:15.196556Z",
     "start_time": "2025-05-12T07:51:15.190933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResBlock(nn.Module):\n",
    "    C: int\n",
    "    num_groups: int\n",
    "    dropout_prob: float\n",
    "\n",
    "    def setup(self):\n",
    "        self.relu = nn.relu\n",
    "        self.gnorm1 = nn.GroupNorm(num_groups=self.num_groups)  # Esta capa normaliza los canales dividiendolas previamente en pequeños grupos\n",
    "        self.gnorm2 = nn.GroupNorm(num_groups=self.num_groups)  # Esta capa normaliza los canales dividiendolas previamente en pequeños grupos\n",
    "        self.conv1 = nn.Conv(self.C, kernel_size=(3, 3), strides=(1, 1), padding=\"SAME\")\n",
    "        self.conv2 = nn.Conv(self.C, kernel_size=(3, 3), strides=(1, 1), padding=\"SAME\")\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "    def __call__(self, x, embeddings, train):\n",
    "        x = x + embeddings[:, :, :, :x.shape[-1]]  # Embeddings vendrá definido por SinusoidalEmbeddings en un \"momento\" determinado de la secuencia de tiempo\n",
    "        r = self.conv1(self.relu(self.gnorm1(x)))\n",
    "        r = self.dropout(r, deterministic=not train)\n",
    "        r = self.conv2(self.relu(self.gnorm2(r)))\n",
    "        return r + x"
   ],
   "id": "97a85d872c3c880f",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://api.wandb.ai/files/wandb_fc/images/projects/605819/472b8f50.png\" width=500, heigh=500 />\n",
   "id": "1e903b92944bd7c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Una vez definido el bloque esencial de nuezstra U-NET, podemos pasar a definir cada capa de nuestra red neuronal. Estas capas estarán compuestas por dos bloques residuales (previamente definidos), una capa de atención basada en transformadores visuales, y una capa de convolución/convolución transpuesta (esto dependerá de si esta capa se encuentra en la primera o en la segunda mitad de nuestra U-NET tal y como veremos más adelante):",
   "id": "1f27dff98d89e4d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T07:51:15.244884Z",
     "start_time": "2025-05-12T07:51:15.240372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UnetLayer(nn.Module):\n",
    "    upscale: bool\n",
    "    attention: bool\n",
    "    num_groups: int\n",
    "    dropout_prob: float\n",
    "    num_heads: int\n",
    "    C: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.ResBlock1 = ResBlock(C=self.C, num_groups=self.num_groups, dropout_prob=self.dropout_prob)\n",
    "        self.ResBlock2 = ResBlock(C=self.C, num_groups=self.num_groups, dropout_prob=self.dropout_prob)\n",
    "        if self.upscale:\n",
    "            # Primera opción: Operación de convolución tranpuesta \"similar\" a PyTorch\n",
    "            self.conv = nn.ConvTranspose(self.C // 2, kernel_size=(4, 4), strides=(2, 2), transpose_kernel=True, padding=\"SAME\")\n",
    "\n",
    "            # Segunda opción: Incrementar el tamaño de la imagen y después hacer una convolución estandar\n",
    "            # self.conv = nn.Conv(self.C // 2, kernel_size=(4, 4), strides=(1, 1), padding=\"SAME\")\n",
    "        else:\n",
    "            self.conv = nn.Conv(self.C * 2, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\")\n",
    "        if self.attention:\n",
    "            self.attention_layer = Attention(self.C, num_heads=self.num_heads, dropout_prob=self.dropout_prob)\n",
    "\n",
    "    def __call__(self, x, embeddings, train):\n",
    "        x = self.ResBlock1(x, embeddings, train)\n",
    "        if hasattr(self, 'attention_layer'):\n",
    "            x = self.attention_layer(x, train)\n",
    "        x = self.ResBlock2(x, embeddings, train)\n",
    "        # if self.upscale:\n",
    "        #     x_conv = jax.image.resize(x, (x.shape[0], 2 * x.shape[1], 2 * x.shape[2], x.shape[3]),  method='nearest')\n",
    "        #     x_conv = self.conv(x_conv)\n",
    "        # else:\n",
    "        #     x_conv = self.conv(x)\n",
    "        x_conv = self.conv(x)\n",
    "        return x_conv, x"
   ],
   "id": "2bec573c4cac683f",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para completar la capa de nuestra U-NET, tenemos que definir la capa con mecanismo de atención. Para más información sobre el mecanismo de funcionamiento de esta capa, es recomendable dirigirse a las diapositivas teóricas del curso.\n",
    "\n",
    "A modo de resumen, la capa de atención se basa en calcular una sería de matrices entrenables a partir de los inputs:\n",
    " - Capa **Q** (Queries): Representa el conjunto de elementos a partir de los cuales calcularemos la atención.\n",
    " - Capa **K** (Keys): Una capa que contiene \"identificadores\" para un determinado valor. Se usa en conjunto con la capa **Q** para determinar cuanta atención hay que prestar a cada valor.\n",
    " - Capa **V** (Values): Esta capa contiene los valores con información (es decir, nuestro input) a los que aplicaremos la atención previamente calculada.\n",
    "\n",
    "Nuestra capa de atención calculará las matrices Q, K y V a partir de los inputs de la capa, aplicando una capa densa (que se puede entender como una proyección de los inputs en base a una matriz entrenable). Previamente a este cálculo, redimensionaremos nuestro input fusionando las dimensiones espaciales de la imagen (generando así una secuencia 1D sobre la que aplicar la atención) y dejando la capa de canales como una capa auxiliar para calcular las features necesarias para derivar las matrices Q, K y V.\n",
    "\n",
    "Una vez calculadas estas matrices, calcularemos la atención de acuerdo a la fórmula:\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*sXEtwjKCACQ6yfW5T8UolQ.png\" width=500, heigh=500 />\n"
   ],
   "id": "dcff3fcd83a3b2cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T07:51:15.293190Z",
     "start_time": "2025-05-12T07:51:15.289518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    C: int\n",
    "    num_heads: int\n",
    "    dropout_prob: float\n",
    "\n",
    "    def setup(self):\n",
    "        self.proj1 = nn.Dense(self.C * 3)\n",
    "        self.proj2 = nn.Dense(self.C)\n",
    "\n",
    "    def __call__(self, x, train):\n",
    "        # rng = jnr.key(random.randint(0, 2**32-1))\n",
    "\n",
    "        h, w = x.shape[1], x.shape[2]\n",
    "        x = rearrange(x, 'b h w c -> b (h w) c')\n",
    "        x = self.proj1(x)\n",
    "        x = rearrange(x, 'b L (C H K) -> K b H L C', K=3, H=self.num_heads)\n",
    "        q, k, v = x[0], x[1], x[2]\n",
    "        # x = nn.dot_product_attention(q, k, v, dropout_rate=self.dropout_prob, dropout_rng=rng, deterministic=not train)\n",
    "        x = nn.dot_product_attention(q, k, v)\n",
    "        x = rearrange(x, 'b H (h w) C -> b h w (H C)', h=h, w=w)\n",
    "        x = self.proj2(x)\n",
    "        return x"
   ],
   "id": "e3d81fc911775ff3",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A partir de las capas previamente definidas, podemos construir nuestra U-Net. Una arquitectura U-NET se basa en una serie de capas simétricas de submuestreado y escalado con conexiones residuales, de tal manera que la salida de la red tenga el mismo tamaño que los datos de entrada. Nuestra clase incluirá un conjunto de listas que nos permitirán definir tanto el número de capas de nuestra U-Net como el encrustamiento de capas de atención solo en determinadas capas.",
   "id": "39c5d5c362f4c88d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T07:51:15.345494Z",
     "start_time": "2025-05-12T07:51:15.339604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UNET(nn.Module):\n",
    "    Channels: List[int] = field(default_factory=lambda: [64, 128, 256, 512, 512, 384])\n",
    "    Attentions: List[bool] = field(default_factory=lambda: [False, True, False, False, False, True])\n",
    "    Upscales: List[bool] = field(default_factory=lambda: [False, False, False, True, True, True])\n",
    "    num_groups: int = 32\n",
    "    dropout_prob: float = 0.1\n",
    "    num_heads: int = 8\n",
    "    output_channels: int = 1\n",
    "    time_steps: int = 1000\n",
    "\n",
    "    def setup(self):\n",
    "        self.num_layers = len(self.Channels)\n",
    "        self.shallow_conv = nn.Conv(self.Channels[0], kernel_size=(3, 3), strides=(1, 1), padding=\"SAME\")\n",
    "        out_channels = (self.Channels[-1] // 2) + self.Channels[0]\n",
    "        self.late_conv = nn.Conv(out_channels // 2, kernel_size=(3, 3), strides=(1, 1), padding=\"SAME\")\n",
    "        self.output_conv = nn.Conv(self.output_channels, kernel_size=(1, 1), strides=(1, 1), padding=\"VALID\")\n",
    "        self.relu = nn.relu\n",
    "        self.embeddings = SinusoidalEmbeddings(time_steps=self.time_steps, embed_dim=max(self.Channels))\n",
    "        for i in range(self.num_layers):\n",
    "            layer = UnetLayer(\n",
    "                upscale=self.Upscales[i],\n",
    "                attention=self.Attentions[i],\n",
    "                num_groups=self.num_groups,\n",
    "                dropout_prob=self.dropout_prob,\n",
    "                C=self.Channels[i],\n",
    "                num_heads=self.num_heads,\n",
    "            )\n",
    "            setattr(self, f'Layer{i+1}', layer)\n",
    "\n",
    "\n",
    "    def __call__(self, x, t, train):\n",
    "        x = self.shallow_conv(x)\n",
    "        residuals = []\n",
    "        for i in range(self.num_layers//2):\n",
    "            layer = getattr(self, f'Layer{i+1}')\n",
    "            embeddings = self.embeddings(t)\n",
    "            x, r = layer(x, embeddings, train)\n",
    "            residuals.append(r)\n",
    "        for i in range(self.num_layers//2, self.num_layers):\n",
    "            layer = getattr(self, f'Layer{i+1}')\n",
    "            x = jnp.concat((layer(x, embeddings, train)[0], residuals[self.num_layers-i-1]), axis=-1)\n",
    "        return self.output_conv(self.relu(self.late_conv(x)))"
   ],
   "id": "91acb9a6f4e6a911",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En este punto, tenemos nuestra red completamente definida y lista para ser entrenada. Sin embargo, aún no tenemos un método para corromper nuestras imágenes con diferentes niveles de ruido dependiendo del momento en el \"tiempo\" en el que se encuentren nuestras imágenes. Recordemos que este \"momento en el tiempo\" hacer referencia al nivel de ruido en las imágenes (siendo t=0 el momento en el que solo hay ruido y t=N el momento en el que nuestra imagen no tiene ruido).\n",
    "\n",
    "Para ello, vamos a definir un ``Scheduler``, el cual nos permitirá recuperar los parámetros necesarios para recuperar el nivel de ruido que corromperá a la imagen en cada momento en el tiempo:"
   ],
   "id": "16a38ae6389d41b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T07:51:15.392329Z",
     "start_time": "2025-05-12T07:51:15.389383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DDPM_Scheduler:\n",
    "    def __init__(self, num_time_steps: int=1000):\n",
    "        self.beta = jnp.linspace(1e-4, 0.02, num_time_steps)\n",
    "        alpha = 1 - self.beta\n",
    "        self.alpha = jnp.cumprod(alpha, axis=0)\n",
    "\n",
    "    def __call__(self, t):\n",
    "        return self.beta[t], self.alpha[t]"
   ],
   "id": "78bd69360a834738",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Una vez definidos todos los componentes necesarios para entrenar la red, podemos proceder con la instanciación del loop de entrenamiento. En ese caso, lo encapsularemos dentro de una función para que sea más sencillo de ejecutar en diferentes scripts.\n",
    "\n",
    "Nuestra función de entrenamiento recibirá como input los principales hiperparámetros a controlar para asegurar que el entrenamiento se ajusta a nuestro requirimientos de hardware. Estos parámetros incluyen:\n",
    "\n",
    "- Número de \"momentos en el tiempo\" (**num_time_steps**): Permite determinar cuantos niveles de ruido habrá entre el primer momento en el tiempo (imagen formada solo por ruido) y el tiempo final (imagen sin ruido)\n",
    "- Número de epochs (**num_epochs**): Permite modificar el número de veces que se recorre el conjunto de entrenamiento\n",
    "- Archivo de punto de guardado (**checkpoint_path**): Permite dar el path a un archivo dónde iremos guardando el estado de la red a medida que el entrenamiento avanza. Esto es muy útil sobre todo cuando el proceso de entrenamiento es largo o queremos analizar el estado de la red en algún punto intermedio del entrenamiento, permitiéndonos recargar el estado de la red en ese punto para hacer inferencia o continuar el entrenamiento desde ese punto."
   ],
   "id": "501bef8125000033"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T07:51:15.527463Z",
     "start_time": "2025-05-12T07:51:15.448753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def init_train_state(model, random_key, dummy_input, learning_rate) -> train_state.TrainState:\n",
    "    # Inicialización del modelo\n",
    "    variables = model.init(random_key, *dummy_input)\n",
    "\n",
    "    # Optimizador\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "\n",
    "    # Creación del estado\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn = model.apply,\n",
    "        tx=optimizer,\n",
    "        params=variables['params']\n",
    "    )\n",
    "\n",
    "@jax.jit\n",
    "def train_step(train_state, alpha, x, t, e, random_key) -> train_state.TrainState:\n",
    "    def loss_fn(params):\n",
    "        a = alpha[t][:, None, None, None]\n",
    "        x_noisy = (jnp.sqrt(a) * x) + (jnp.sqrt(1 - a) * e)\n",
    "        output = train_state.apply_fn({'params': params}, x_noisy, t, True, rngs={'dropout': random_key})\n",
    "        loss = jnp.mean((output - e) ** 2)\n",
    "        return loss\n",
    "\n",
    "    gradient_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = gradient_fn(train_state.params)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "\n",
    "    return train_state, loss\n",
    "\n",
    "\n",
    "def train(batch_size: int=64,\n",
    "          num_time_steps: int=1000,\n",
    "          num_epochs: int=15,\n",
    "          lr=2e-5,\n",
    "          checkpoint_path: str=None):\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(\"./MNIST_DATA\", train=True, download=True,transform=transforms.ToTensor())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0, collate_fn=numpy_collate)\n",
    "\n",
    "    scheduler = DDPM_Scheduler(num_time_steps=num_time_steps)\n",
    "\n",
    "    rng = jnr.key(random.randint(0, 2**32-1))  # En JAX hay que instanciar una random seed manualmente\n",
    "    rng, model_key = jax.random.split(rng, 2)\n",
    "\n",
    "    model = UNET()\n",
    "    state = init_train_state(model, model_key, [jnp.zeros((1, 32, 32, 1)), jnp.array([0], dtype=jnp.int32), True], lr)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for bidx, (x,_) in enumerate(tqdm(train_loader, desc=f\"Epoch {i + 1}/{num_epochs}\")):\n",
    "            rng, t_key, noise_key, step_key = jnr.split(rng, 4)\n",
    "            x = rearrange(x, 'b c w h -> b w h c')\n",
    "            x = jnp.pad(x, ((0, 0), (2, 2), (2, 2), (0, 0)))\n",
    "            t = jnr.randint(minval=0, maxval=num_time_steps, shape=(batch_size,), key=t_key)\n",
    "            e = jnr.normal(shape=x.shape, key=noise_key)\n",
    "            # a = scheduler.alpha[t][:, None, None, None]\n",
    "            # x = (jnp.sqrt(a) * x) + (jnp.sqrt(1 - a) * e)\n",
    "            state, loss = train_step(state, scheduler.alpha, x, t, e, step_key)\n",
    "            total_loss += loss\n",
    "        print(f'Epoch {i + 1} | Loss {total_loss / (60000 / batch_size):.5f}')\n",
    "\n",
    "    checkpoint = {\n",
    "        'model': state,\n",
    "    }\n",
    "\n",
    "    if checkpoint_path is not None:\n",
    "        orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "        save_args = orbax_utils.save_args_from_target(checkpoint)\n",
    "        orbax_checkpointer.save(os.path.abspath(os.path.join(checkpoint_path, \"checkpoint\")), checkpoint, save_args=save_args)"
   ],
   "id": "6340e8721cf18b53",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "El siguiente paso consiste en entrenar nuestra red usando la función anterior:",
   "id": "827bc99cc35977fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:05:02.700481Z",
     "start_time": "2025-05-12T07:51:15.538696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    if os.path.isdir(os.path.join('checkpoints')):\n",
    "        shutil.rmtree(os.path.join('checkpoints'))\n",
    "    train(checkpoint_path=os.path.join('checkpoints', 'ddpm_checkpoint_jax'), lr=1e-4, num_epochs=5)"
   ],
   "id": "89b6e1e5c4babd19",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 937/937 [02:48<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 0.10118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 937/937 [02:37<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss 0.04491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 937/937 [02:40<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss 0.03493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 937/937 [02:42<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss 0.03006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 937/937 [02:49<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss 0.02722\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Una vez hayamos entrenado la red, podemos aprovechar nuestro archivo ``checkpoint.pth`` para recargarla y hacer inferencia de nuestro conjunto de datos para así comprobar si la red es capaz de regenerar nuestras imágenes a partir de puro ruido. Para ello, definiremos en primer lugar una función para representar las imágenes que se generen desde la red.",
   "id": "bef3ab12f8f963c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:05:02.787742Z",
     "start_time": "2025-05-12T08:05:02.782108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_reverse(images: List, times: List=None, save_gif=True):\n",
    "    if times is not None:\n",
    "        t = 0\n",
    "\n",
    "    if save_gif:\n",
    "        frames = []\n",
    "        fig, ax = plt.subplots()\n",
    "        for i in range(len(images)):\n",
    "            x = images[i].squeeze()\n",
    "            im = ax.imshow(x, animated=True, cmap=\"Grays_r\")\n",
    "            if i == 0:\n",
    "                ax.imshow(x, cmap=\"Grays_r\")\n",
    "            if times is not None:\n",
    "                ax.set_title(\"Time: \" + str(times[t]))\n",
    "                t += 1\n",
    "            ax.axis('off')\n",
    "            fig.tight_layout()\n",
    "            frames.append([im])\n",
    "        ani = animation.ArtistAnimation(fig, frames, interval=200, blit=True,\n",
    "                                        repeat_delay=1000)\n",
    "        writer = animation.PillowWriter(fps=2, bitrate=1800)\n",
    "        ani.save(f\"diffusion_model_example.gif\", writer=writer)\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, len(images), figsize=(len(images), 1))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            x = images[i].squeeze()\n",
    "            ax.imshow(x, cmap=\"Grays\")\n",
    "            if times is not None:\n",
    "                ax.set_title(\"Time: \" + str(times[t]))\n",
    "                t += 1\n",
    "            ax.axis('off')\n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "58abc8e7a4f85884",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A continuación definiremos una nueva función que nos permitirá hacer inferencia con la red. Esta función aprovechará el archivo ``checkpoint.pth`` para recargar el estado de la red y así poder utilizarla para predecir imágenes a partir de puro ruido.\n",
    "\n",
    "Para ello, aprovecharemos nuestro ``DDPM_Scheduler`` para generar los parámetros que necesitaremos para generar los diferentes niveles de ruido. De manera intuitiva, el proceso de inferencia es exactamente igual al de entrenamiento pero en sentido opuesto: empezaremos con imágenes de ruido y progresivamente la red irá disminuyendo el ruido hasta conseguir una imagen única que se parezca a las imágenes de entrenamiento. Cada vez que generemos una nueva imagen de ruido, obtendremos una imagen única diferente."
   ],
   "id": "677c1cbf74c7d3fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:05:02.986533Z",
     "start_time": "2025-05-12T08:05:02.876991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@jax.jit\n",
    "def eval_step(state, x, t):\n",
    "    y = state.apply_fn({'params': state.params}, x, t, False)\n",
    "    return y\n",
    "\n",
    "def inference(checkpoint_path: str=None,\n",
    "              num_time_steps: int=1000):\n",
    "\n",
    "    rng = jnr.key(random.randint(0, 2**32-1))  # En JAX hay que instanciar una random seed manualmente\n",
    "    rng, model_key = jax.random.split(rng, 2)\n",
    "\n",
    "    model = UNET()\n",
    "    variables = model.init(model_key, *[jnp.zeros((1, 32, 32, 1)), jnp.array([0], dtype=jnp.int32), True])\n",
    "    state = train_state.TrainState.create(apply_fn=model.apply, params=jax.tree_util.tree_map(np.zeros_like, variables['params']), tx=optax.adam(1.))\n",
    "\n",
    "    orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "    target = {'model': state,}\n",
    "    state = orbax_checkpointer.restore(os.path.abspath(os.path.join(checkpoint_path, \"checkpoint\")), item=target)[\"model\"]\n",
    "\n",
    "    scheduler = DDPM_Scheduler(num_time_steps=num_time_steps)\n",
    "    times = [0,15,50,100,200,300,400,550,700,999]\n",
    "    images = []\n",
    "\n",
    "    for i in range(1):\n",
    "        rng, img_rng = jnr.split(rng, 2)\n",
    "        z = jnr.normal(key=rng, shape=(1, 32, 32, 1))\n",
    "        for t in reversed(range(1, num_time_steps)):\n",
    "            img_rng, noise_key = jnr.split(img_rng, 2)\n",
    "            t = jnp.array([t])\n",
    "            temp = (scheduler.beta[t] / ( (jnp.sqrt( 1 - scheduler.alpha[t])) * (jnp.sqrt(1 - scheduler.beta[t]))))\n",
    "            z = (1 / (jnp.sqrt(1 - scheduler.beta[t]))) * z - (temp * eval_step(state, z, t))\n",
    "            if t[0] in times:\n",
    "                images.append(np.array(z))\n",
    "            e = jnr.normal(key=noise_key, shape=(1, 32, 32, 1))\n",
    "            z = z + (e * jnp.sqrt(scheduler.beta[t]))\n",
    "        temp = scheduler.beta[0] / ((jnp.sqrt(1 - scheduler.alpha[0])) * (jnp.sqrt(1 - scheduler.beta[0])))\n",
    "        x = (1 / (jnp.sqrt(1 - scheduler.beta[0]))) * z - (temp * eval_step(state, z, jnp.array([0])))\n",
    "\n",
    "        images.append(np.array(x))\n",
    "        display_reverse(images, times=list(reversed(times)))\n",
    "        images = []"
   ],
   "id": "f4d5f7cd21fcb20e",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A continuación, probaremos la capacidad de inferencia de la red:",
   "id": "6aa39e3bd2ac8dd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:05:36.223355Z",
     "start_time": "2025-05-12T08:05:02.997918Z"
    }
   },
   "cell_type": "code",
   "source": "inference(os.path.join('checkpoints', 'ddpm_checkpoint_jax'))",
   "id": "3019333251290ffc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dherreros/miniforge3/envs/pytorch/lib/python3.12/site-packages/orbax/checkpoint/_src/serialization/type_handlers.py:1251: UserWarning: Sharding info not provided when restoring. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"diffusion_model_example.gif\" width=500, heigh=500 />",
   "id": "54fee1589c919b44"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
